{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20960215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Graphics\n"
     ]
    }
   ],
   "source": [
    "#HERE TO TEST IDEA FROM LINE CONVERSATION\n",
    "import numpy as np                # import numpy\n",
    "import matplotlib.pyplot as plt   # import matplotlib, a python 2d plotting library\n",
    "from tqdm import tqdm\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "\n",
    "#import torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print('Running on Graphics')\n",
    "  device=torch.device('cuda:0')\n",
    "else:\n",
    "  device=torch.device('cpu')\n",
    "  print('Running on Processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cd0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1,8,3, padding=1)\n",
    "        self.c2 = nn.Conv2d(8,16,3, padding=1)\n",
    "        self.c3 = nn.Conv2d(16,32,3, padding=1)\n",
    "        self.l = nn.Linear(32,10)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool(self.act(self.c1(x)))\n",
    "        self.feat = self.pool(self.act(self.c2(x)))\n",
    "        x = self.avgpool(self.act(self.c3(self.feat))).flatten(start_dim=1)\n",
    "        x = self.l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870bed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST('../mnist_digits/', train=True, download=True,transform=torchvision.transforms.ToTensor())\n",
    "test_data = MNIST('../mnist_digits/', train=False, download=True,transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65e28369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optim, train_data, test_data, models, epochs, batch_size, target):\n",
    "    metrics = []\n",
    "    n = len(test_data)\n",
    "    for i in tqdm(range(epochs)):\n",
    "        for idx, (x, y) in enumerate(DataLoader(train_data, batch_size=batch_size, shuffle=True)):\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "          yhat1 = models[0](x)\n",
    "          yhat2 = models[1](x)\n",
    "          for model in models:\n",
    "              model.zero_grad()\n",
    "          loss = get_loss(y, yhat1, yhat2, models, target)\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "        t_acc = torch.zeros(2, dtype=torch.float32)\n",
    "        for idx, (x, y) in enumerate(DataLoader(test_data, batch_size=batch_size)):\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "          for j in range(len(models)):\n",
    "              y_hat = models[j](x)\n",
    "              t_acc[j] = t_acc[j] + torch.sum(torch.argmax(y_hat, dim=1)==y)\n",
    "        t_acc = t_acc/n\n",
    "        metrics.append(t_acc)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed7524da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = 0.00001\n",
    "loss_ce = nn.CrossEntropyLoss()\n",
    "def get_loss(y,yhat1,yhat2, models, target): #Use as a loss function\n",
    "    L1 = loss_ce(yhat1,y)\n",
    "    L2 = loss_ce(yhat2,y)\n",
    "    L3 = torch.tensor(0)\n",
    "    a = list(models[0].parameters())\n",
    "    b = list(models[1].parameters())\n",
    "    for j in range(len(list(models[0].parameters()))):\n",
    "        L3 = L3 + torch.sum(torch.square(a[j] - b[j]))  \n",
    "    #print('L1:{} L2:{} L3:{}'.format(L1,L2,L3))\n",
    "    return L1+L2+lambda1*torch.square(L3-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af89e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get2classfiers(epochs, target): #Trains 2 classifiers in parallel with the target parametric distance\n",
    "    cls1 = Classifier().to(device)\n",
    "    cls2 = Classifier().to(device)\n",
    "    optimizer = optim.Adam(list(cls1.parameters())+list(cls2.parameters()), lr = 5.0e-3)\n",
    "    loss_ce = nn.CrossEntropyLoss()\n",
    "    models = [cls1, cls2]\n",
    "    metric = train(optimizer, train_data, test_data, models, epochs, batch_size, target)\n",
    "    #print(metric)\n",
    "    return cls1, cls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf7ca798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_FGSM(x, y, eta, model): #Generates FGSM adversarial samples\n",
    "    model.zero_grad()\n",
    "    x.requires_grad = True\n",
    "    y_hat = model(x)\n",
    "    loss = loss_ce(y_hat, y)\n",
    "    loss.backward()\n",
    "    perturbed_x = torch.clamp(x + eta*(x.grad.data).sign(), min=0, max=1.0)\n",
    "    return perturbed_x#, x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2455b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find mutual residual (proxy for mutual information)\n",
    "def get_R(X,Y):\n",
    "    X = torch.flatten(X, start_dim=1)\n",
    "    Y = torch.flatten(Y, start_dim=1)\n",
    "    #First modify to create nonsingular X:\n",
    "    _,R = torch.linalg.qr(X)\n",
    "    cols = torch.diag(R)\n",
    "    cols = abs(cols/torch.max(cols))>0.0005\n",
    "    X = X[:,cols]\n",
    "\n",
    "    X = torch.cat([X, torch.ones([batch_size,1]).to(device)],dim=1)\n",
    "    Yhat = torch.matmul(torch.matmul(X,torch.linalg.pinv(X)),Y)\n",
    "    #Yhat = torch.matmul(torch.matmul(X,get_pinv(X, q)), Y)\n",
    "    Ehat = Y - Yhat\n",
    "    SSres = torch.sum(torch.square(Ehat))\n",
    "    Ybar = torch.mean(Y, dim=0).unsqueeze(0)\n",
    "    SStot = torch.sum(torch.square(Y-Ybar))\n",
    "    eta = 0.001 #constant for stability\n",
    "    R = 1 - SSres/(SStot+eta)\n",
    "    return 1-SSres/SStot #torch.log(SStot+eta)-torch.log(SSres+eta) #R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df75bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer(targets, etas):\n",
    "    trans_rate = np.zeros([len(targets), len(etas)])\n",
    "    R2 = np.zeros([len(targets), len(etas)])\n",
    "    for tidx, target in enumerate(targets):\n",
    "        cls1, cls2 = get2classfiers(epochs, target)\n",
    "        for eidx, eta in enumerate(etas):\n",
    "            for x,y in DataLoader(test_data, batch_size, shuffle=True):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                x = gen_FGSM(x,y,eta,cls1)\n",
    "                c1=(torch.argmax(cls1(x), dim=1)==y)\n",
    "                c2=(torch.argmax(cls2(x), dim=1)==y)\n",
    "                adv_tran = torch.sum(~c1 & ~c2)/torch.sum(~c1)\n",
    "                trans_rate[tidx,eidx] = trans_rate[tidx,eidx] + adv_tran#.detach().cpu().numpy()\n",
    "                R2[tidx,eidx] = R2[tidx,eidx] + get_R(cls1.feat, cls2.feat)\n",
    "    trans_rate = trans_rate*batch_size/(len(test_data))\n",
    "    R2 = R2*batch_size/(len(test_data))\n",
    "    \n",
    "    return trans_rate, R2\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e043dc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.4844, 0.4184]), tensor([0.7726, 0.7767]), tensor([0.8581, 0.8782]), tensor([0.8923, 0.9061]), tensor([0.9079, 0.9194]), tensor([0.9172, 0.9281]), tensor([0.9250, 0.9400]), tensor([0.9334, 0.9382]), tensor([0.9282, 0.9457]), tensor([0.9441, 0.9538])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:39<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.3605, 0.3857]), tensor([0.7788, 0.7471]), tensor([0.8579, 0.8583]), tensor([0.9016, 0.8798]), tensor([0.9141, 0.9005]), tensor([0.9201, 0.9148]), tensor([0.9330, 0.9162]), tensor([0.9340, 0.9247]), tensor([0.9443, 0.9341]), tensor([0.9502, 0.9330])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.2788, 0.2874]), tensor([0.6293, 0.7081]), tensor([0.8308, 0.8521]), tensor([0.8813, 0.8840]), tensor([0.9133, 0.9093]), tensor([0.9301, 0.9216]), tensor([0.9353, 0.9294]), tensor([0.9402, 0.9358]), tensor([0.9487, 0.9337]), tensor([0.9540, 0.9437])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.2577, 0.2896]), tensor([0.6796, 0.5628]), tensor([0.7928, 0.7589]), tensor([0.8707, 0.8456]), tensor([0.8782, 0.8787]), tensor([0.8962, 0.8914]), tensor([0.9128, 0.9078]), tensor([0.9260, 0.9277]), tensor([0.9241, 0.9345]), tensor([0.9399, 0.9381])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.2231, 0.2193]), tensor([0.5888, 0.5722]), tensor([0.8040, 0.7796]), tensor([0.8465, 0.8492]), tensor([0.8863, 0.8667]), tensor([0.9015, 0.8951]), tensor([0.9196, 0.9079]), tensor([0.9208, 0.9083]), tensor([0.9340, 0.9185]), tensor([0.9403, 0.9215])]\n"
     ]
    }
   ],
   "source": [
    "targets = [1000,1500,2000,2500,3000]\n",
    "etas = [0.0, 0.05, 0.1, 0.15]\n",
    "epochs=10\n",
    "batch_size=1000\n",
    "trans_rate, R2 = get_transfer(targets, etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50c77b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('cnn_transfer.npz', trans_rate, R2, np.array(targets), np.array(etas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
